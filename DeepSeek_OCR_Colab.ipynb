{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-OCR Testing Notebook\n",
    "\n",
    "**Setup Guide**: This notebook tests DeepSeek-OCR on Google Colab Pro\n",
    "\n",
    "## Requirements\n",
    "- Google Colab Pro (for guaranteed GPU access)\n",
    "- ~15-20 minutes first run (model download)\n",
    "- ~2 minutes subsequent runs (cached)\n",
    "\n",
    "## What This Does\n",
    "1. ‚úÖ Checks GPU availability\n",
    "2. ‚úÖ Installs dependencies (CUDA-enabled PyTorch, flash-attention)\n",
    "3. ‚úÖ Downloads DeepSeek-OCR model (~8GB)\n",
    "4. ‚úÖ Tests with sample images\n",
    "5. ‚úÖ Processes your own images\n",
    "\n",
    "## Instructions\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** (or better)\n",
    "2. **Runtime ‚Üí Run all**\n",
    "3. Wait for completion (progress bars will show)\n",
    "4. Upload your images in the last cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU found! Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Clone DeepSeek-OCR Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (includes sample images)\n",
    "!git clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n",
    "%cd DeepSeek-OCR\n",
    "!ls -la assets/  # Show sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Install Dependencies\n",
    "\n",
    "This will take ~5-10 minutes. Installing:\n",
    "- PyTorch 2.6.0 with CUDA 11.8\n",
    "- Flash Attention 2.7.3\n",
    "- Transformers, PyMuPDF, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA 11.8\n",
    "!pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install other requirements\n",
    "!pip install -q transformers==4.46.3 tokenizers==0.20.3 PyMuPDF img2pdf einops easydict addict Pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flash Attention (takes ~3-5 minutes to build)\n",
    "print(\"‚öôÔ∏è  Building Flash Attention... This takes 3-5 minutes\")\n",
    "!pip install -q flash-attn==2.7.3 --no-build-isolation\n",
    "print(\"‚úÖ Flash Attention installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load DeepSeek-OCR Model\n",
    "\n",
    "**First run**: Downloads ~8GB model (5-10 minutes)\n",
    "\n",
    "**Subsequent runs**: Uses cached model (30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üì• Loading DeepSeek-OCR model...\")\n",
    "print(\"‚è≥ First run: ~5-10 min download | Cached runs: ~30 sec\")\n",
    "\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    _attn_implementation='flash_attention_2',\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Move to GPU and set to eval mode\n",
    "model = model.eval().cuda().to(torch.bfloat16)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üéØ Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test with Sample Images\n",
    "\n",
    "Run OCR on the included sample images to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display sample image\n",
    "image_path = 'assets/show1.jpg'\n",
    "img = Image.open(image_path)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Sample Image')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìê Image size: {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Convert Document to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Running OCR with Markdown conversion...\\n\")\n",
    "\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown.\"\n",
    "output_path = './output'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "result = model.infer(\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    image_file=image_path,\n",
    "    output_path=output_path,\n",
    "    base_size=1024,      # Base resolution\n",
    "    image_size=640,      # Crop size\n",
    "    crop_mode=True,      # Dynamic resolution (Gundam mode)\n",
    "    save_results=True,\n",
    "    test_compress=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìÑ RESULT:\")\n",
    "print(\"=\"*50)\n",
    "print(result)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Free OCR (Text Only, No Layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Running Free OCR (text only)...\\n\")\n",
    "\n",
    "prompt = \"<image>\\nFree OCR.\"\n",
    "\n",
    "result = model.infer(\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    image_file=image_path,\n",
    "    output_path=output_path,\n",
    "    base_size=1024,\n",
    "    image_size=640,\n",
    "    crop_mode=True,\n",
    "    save_results=False,\n",
    "    test_compress=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìÑ RESULT:\")\n",
    "print(\"=\"*50)\n",
    "print(result)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Describe Image in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Describing image...\\n\")\n",
    "\n",
    "prompt = \"<image>\\nDescribe this image in detail.\"\n",
    "\n",
    "result = model.infer(\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    image_file=image_path,\n",
    "    output_path=output_path,\n",
    "    base_size=1024,\n",
    "    image_size=640,\n",
    "    crop_mode=True,\n",
    "    save_results=False,\n",
    "    test_compress=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìÑ RESULT:\")\n",
    "print(\"=\"*50)\n",
    "print(result)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Process Your Own Images\n",
    "\n",
    "Upload your images here and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload images\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"üì§ Click 'Choose Files' to upload your images...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(f\"\\n‚úÖ Uploaded {len(uploaded)} file(s)\")\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"  - {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process uploaded images\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "PROMPT = \"<image>\\n<|grounding|>Convert the document to markdown.\"  # Change this!\n",
    "OUTPUT_DIR = './user_outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Available prompts:\n",
    "# \"<image>\\n<|grounding|>Convert the document to markdown.\"\n",
    "# \"<image>\\n<|grounding|>OCR this image.\"\n",
    "# \"<image>\\nFree OCR.\"\n",
    "# \"<image>\\nParse the figure.\"\n",
    "# \"<image>\\nDescribe this image in detail.\"\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÑ Processing: {filename}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Display image\n",
    "    img = Image.open(io.BytesIO(uploaded[filename]))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save temporarily\n",
    "    temp_path = f'/tmp/{filename}'\n",
    "    with open(temp_path, 'wb') as f:\n",
    "        f.write(uploaded[filename])\n",
    "    \n",
    "    # Run inference\n",
    "    result = model.infer(\n",
    "        tokenizer,\n",
    "        prompt=PROMPT,\n",
    "        image_file=temp_path,\n",
    "        output_path=OUTPUT_DIR,\n",
    "        base_size=1024,\n",
    "        image_size=640,\n",
    "        crop_mode=True,\n",
    "        save_results=True,\n",
    "        test_compress=True\n",
    "    )\n",
    "    \n",
    "    # Save result\n",
    "    result_file = os.path.join(OUTPUT_DIR, f\"{filename}_result.md\")\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved to: {result_file}\")\n",
    "    print(f\"\\nüìÑ RESULT:\\n{'-'*60}\")\n",
    "    print(result[:1000])  # Show first 1000 chars\n",
    "    if len(result) > 1000:\n",
    "        print(f\"\\n... (truncated, see full result in {result_file})\")\n",
    "\n",
    "print(f\"\\n\\nüéâ All images processed! Results in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download all results\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    # Create zip file\n",
    "    shutil.make_archive('deepseek_ocr_results', 'zip', OUTPUT_DIR)\n",
    "    \n",
    "    # Download\n",
    "    print(\"üì¶ Downloading results...\")\n",
    "    files.download('deepseek_ocr_results.zip')\n",
    "    print(\"‚úÖ Downloaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to download yet. Run the processing cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Resolution Modes\n",
    "\n",
    "You can adjust quality vs speed by changing these parameters:\n",
    "\n",
    "```python\n",
    "# Tiny: Fastest, lowest quality (64 vision tokens)\n",
    "base_size=512, image_size=512, crop_mode=False\n",
    "\n",
    "# Small: Fast, good quality (100 vision tokens)\n",
    "base_size=640, image_size=640, crop_mode=False\n",
    "\n",
    "# Base: Balanced (256 vision tokens)\n",
    "base_size=1024, image_size=1024, crop_mode=False\n",
    "\n",
    "# Large: Slow, best quality (400 vision tokens)\n",
    "base_size=1280, image_size=1280, crop_mode=False\n",
    "\n",
    "# Gundam: Dynamic, best for documents (256 + n√ó100 tokens)\n",
    "base_size=1024, image_size=640, crop_mode=True  # ‚Üê Default\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Common Prompts\n",
    "\n",
    "```python\n",
    "# Documents\n",
    "\"<image>\\n<|grounding|>Convert the document to markdown.\"\n",
    "\n",
    "# General OCR\n",
    "\"<image>\\n<|grounding|>OCR this image.\"\n",
    "\n",
    "# Text only (no layout)\n",
    "\"<image>\\nFree OCR.\"\n",
    "\n",
    "# Figures in documents\n",
    "\"<image>\\nParse the figure.\"\n",
    "\n",
    "# General description\n",
    "\"<image>\\nDescribe this image in detail.\"\n",
    "\n",
    "# Locate specific text\n",
    "\"<image>\\nLocate <|ref|>xxxx<|/ref|> in the image.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Tips\n",
    "\n",
    "- **First run**: Takes 15-20 min (model download)\n",
    "- **Subsequent runs**: 2-3 min (cached)\n",
    "- **GPU memory**: Uses ~12-16 GB\n",
    "- **Best results**: Use `crop_mode=True` for documents\n",
    "- **Faster inference**: Use smaller `base_size` values\n",
    "\n",
    "---\n",
    "\n",
    "**Created by**: Carlos Lorenzo Santos\n",
    "\n",
    "**Date**: October 2025\n",
    "\n",
    "**Model**: [deepseek-ai/DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
